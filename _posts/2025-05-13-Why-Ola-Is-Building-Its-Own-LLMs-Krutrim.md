---
layout: post
title: "Why Ola Is Building Its Own LLMs (Krutrim)"
date: 2025-05-13
excerpt: "Ola is no longer just a mobility company. It is positioning itself as a data and AI company, and Krutrim is a key part of that shift."
---

Ola started as a mobility services company, but today it is aiming for something much bigger. It is no longer just about cabs. Ola is evolving into a **data and AI company**.

Building its own large language models (LLMs), branded as **Krutrim**, is part of this broader vision. More importantly, it shows how Indian companies can shape the future of AI **on their own terms**, rather than importing it wholesale.

This way of thinking is inspired by the framework from the book *Fusion Strategy*.  
Believe me, this is far beyond a mobility platform. Let me explain.

![Ola](\assets\images\Ola\mobility.png)

## Ola Is Not Just Uber’s Indian Version

We often compare Ola with Uber. But a more accurate comparison today is between **Ola Electric and Tesla**.

Why?

Around **2017**, Ola entered the electric vehicle (EV) space. Today, it operates across:
- Ride-hailing
- EV manufacturing
- Battery and charging infrastructure
- AI and data platforms

This mirrors Tesla’s strategy of building a **data-driven mobility ecosystem**, not just vehicles.

---

## Data Network Effects: The Real Asset

A **data network effect** happens when a system gets better the more people use it, because each interaction generates data that improves the product.

A simple example:  
Google Maps becomes better at predicting traffic as more users share live location data.

Here’s the key difference:

- Tesla collects data from privately owned vehicles.
- **Ola already operates a massive ride-hailing fleet**, generating real-time motion, routing, demand, and behavioral data at scale.

This gives Ola a unique advantage:
- Fleet-level data
- Continuous usage
- Dense urban mobility signals

That data can be used to:
- Optimize EV performance
- Improve routing and charging
- Train AI systems for prediction, personalization, and automation

---

## India’s Unfair Advantage

India’s population creates a structural advantage:
- More users
- More two-wheelers
- Higher ride frequency
- Denser mobility data

This is also why Ola focused heavily on **two-wheelers instead of four-wheelers**. The data density per rupee spent is much higher.

If **data is the new oil**, then it makes sense to also build the **engine** that runs on that oil — not borrow someone else’s.

That engine is the **model architecture**.

---

## The Flywheel Ola Is Building

Ola is quietly creating a powerful flywheel:

Cab-sharing → EV adoption → more data → India-specific AI model training

Each loop strengthens the next.

If companies like Uber don’t adapt to a data-first, model-owning strategy, they risk becoming infrastructure providers rather than intelligence owners.

---

## Why Not Just Fine-Tune Open-Source Models?

A natural question is:  
Why not just fine-tune models like Mistral or LLaMA?

This is where **architecture control** matters.

Fine-tuning gives partial control, but:
- You are still bound by architectures designed for Western contexts
- You inherit design choices that don’t suit India well

Examples:
- Most tokenizers are optimized for **Latin scripts**
- Indian languages need **Indic-first tokenization**, dialect handling, and code-mixed language support
- Many “open” models (like LLaMA) have **restricted commercial licenses**, limiting redistribution, monetization, and modification

If India wants a long-term AI ecosystem that supports its linguistic and cultural diversity, **owning the foundation model matters**.

---

## What Full Control Enables

Building a native foundation model allows:
- Monetizing APIs on your own terms
- Licensing models to governments, schools, and startups
- Creating a **sovereign AI tech stack**
- Long-term scalability without external constraints

That’s why Ola isn’t just fine-tuning models.

That’s why it’s building one.

---

## Closing Thought

Ola’s move into LLMs is not about hype.  
It’s about **owning the intelligence layer** of mobility, energy, and AI in an Indian context.

Special thanks: I was introduced to this thinking framework by **Michael Driscoll**, who learned it from his mentors **Vijay Govindarajan** and **Venkat Venkatraman**.
